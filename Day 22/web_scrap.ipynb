{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully scraped and saved to 'bu_facts_stats.json'\n"
     ]
    }
   ],
   "source": [
    "import requests  # Import requests to fetch web page content\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup to parse HTML\n",
    "import json  # Import JSON to store the scraped data\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'http://www.bu.edu/president/boston-university-facts-stats/'\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all table elements in the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Initialize an empty list to store table data\n",
    "    data = []\n",
    "    \n",
    "    # Loop through each table found on the page\n",
    "    for table in tables:\n",
    "        table_data = []  # Store individual table rows\n",
    "        \n",
    "        # Find all rows in the table\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        # Loop through each row\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['th', 'td'])  # Extract headers and data columns\n",
    "            cols = [col.text.strip() for col in cols]  # Clean text by stripping whitespace\n",
    "            table_data.append(cols)  # Append the cleaned row data\n",
    "        \n",
    "        data.append(table_data)  # Add table data to the main data list\n",
    "    \n",
    "    # Save the scraped data as a JSON file\n",
    "    with open('bu_facts_stats.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(\"Data successfully scraped and saved to 'bu_facts_stats.json'\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the UCI datasets page\n",
    "url = \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "\n",
    "# Send an HTTP GET request\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Some sites block bots, so a User-Agent is added\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the main table containing dataset information\n",
    "    table = soup.find(\"table\", {\"border\": \"1\"})  # The dataset table has a border attribute\n",
    "    \n",
    "    # Extract table headers\n",
    "    headers = [header.text.strip() for header in table.find_all(\"th\")]\n",
    "    \n",
    "    # Extract table rows\n",
    "    datasets = []\n",
    "    for row in table.find_all(\"tr\")[1:]:  # Skipping the header row\n",
    "        columns = row.find_all(\"td\")\n",
    "        data = {headers[i]: columns[i].text.strip() for i in range(len(columns))}\n",
    "        datasets.append(data)\n",
    "    \n",
    "    # Save data to a JSON file\n",
    "    with open(\"uci_datasets.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(datasets, file, indent=4)\n",
    "    \n",
    "    print(\"Data successfully extracted and saved to uci_datasets.json\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to us_presidents.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the Wikipedia page containing the list of U.S. presidents\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Setting user-agent to avoid blocks\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the webpage content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Locate the first table that contains the list of presidents\n",
    "    table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "    \n",
    "    # Extract table headers\n",
    "    headers = [th.text.strip() for th in table.find_all(\"th\")]\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = []\n",
    "    for row in table.find_all(\"tr\")[1:]:  # Skip the header row\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) > 0:\n",
    "            data = [col.text.strip() for col in columns]\n",
    "            rows.append(dict(zip(headers, data)))\n",
    "    \n",
    "    # Save the extracted data into a JSON file\n",
    "    with open(\"us_presidents.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(rows, file, indent=4)\n",
    "    \n",
    "    print(\"Scraped data saved to us_presidents.json\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status Code:\", response.status_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
