{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 5), ('the', 5), ('gutenberg', 4), ('you', 4), ('project', 3), ('about', 3), ('contact', 3), ('of', 3), ('and', 3), ('help', 3)]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def most_frequent_words(url, top_n=10):\n",
    "    \"\"\"\n",
    "    Fetches and processes text from a given URL, removes HTML tags, filters metadata, \n",
    "    and finds the most frequent words.\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): The URL of the text source.\n",
    "    top_n (int): Number of most common words to return.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tuples with the most common words and their frequencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)  # Get the text from the URL\n",
    "    html_content = response.text  # Extract raw HTML content\n",
    "\n",
    "    # Use BeautifulSoup to remove HTML tags and keep only visible text\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")  # Extract text while preserving spacing\n",
    "\n",
    "    # Gutenberg books have metadata at the beginning and end\n",
    "    start_marker = \"*** START OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\"\n",
    "    end_marker = \"*** END OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\"\n",
    "\n",
    "    # Find where the book starts and ends\n",
    "    start_idx = text.find(start_marker)\n",
    "    end_idx = text.find(end_marker)\n",
    "\n",
    "    if start_idx != -1 and end_idx != -1:\n",
    "        text = text[start_idx + len(start_marker):end_idx]  # Keep only book content\n",
    "\n",
    "    # Use regex to extract words (removing numbers, punctuation, and extra spaces)\n",
    "    words = re.findall(r'\\b[a-zA-Z]{2,}\\b', text.lower())  # Ensures only meaningful words\n",
    "\n",
    "    # Count occurrences and return the top N words\n",
    "    word_counts = Counter(words).most_common(top_n)\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# URL of \"Romeo and Juliet\" from Project Gutenberg\n",
    "romeo_and_juliet_url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "\n",
    "# Call the function and display the 10 most frequent words\n",
    "print(most_frequent_words(romeo_and_juliet_url, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analyze Cats API Data\n",
    "We will:\n",
    "\n",
    "Fetch the cat breeds data\n",
    "Extract weight and lifespan values\n",
    "\n",
    "Compute min, max, mean, median, and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Stats: {'min': 3.0, 'max': 7.5, 'mean': 4.708955223880597, 'median': 4.5, 'std_dev': 1.066533799956462}\n",
      "Lifespan Stats: {'min': 10.5, 'max': 19.0, 'mean': 13.746268656716419, 'median': 13.5, 'std_dev': 1.5844249849048053}\n",
      "Country Frequency: {'Egypt': 3, 'Greece': 1, 'United States': 28, 'United Arab Emirates': 1, 'Australia': 1, 'France': 2, 'United Kingdom': 8, 'Burma': 2, 'Canada': 3, 'Cyprus': 1, 'Russia': 4, 'China': 1, 'Japan': 1, 'Thailand': 4, 'Isle of Man': 1, 'Norway': 1, 'Iran (Persia)': 1, 'Singapore': 1, 'Somalia': 1, 'Turkey': 2}\n"
     ]
    }
   ],
   "source": [
    "import requests  # Library for making HTTP requests\n",
    "import numpy as np  # Library for numerical computations (not used here but typically useful)\n",
    "import statistics  # Library for computing statistical measures\n",
    "\n",
    "# API endpoint for fetching cat breed data\n",
    "cats_api_url = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "def analyze_cats():\n",
    "    \"\"\"\n",
    "    Fetches cat breed data from The Cat API, extracts weight, lifespan, and country frequency, \n",
    "    and computes statistical summaries for weight and lifespan.\n",
    "    \n",
    "    Returns:\n",
    "        - weight_stats (dict): Min, max, mean, median, and standard deviation of cat weights\n",
    "        - lifespan_stats (dict): Min, max, mean, median, and standard deviation of cat lifespan\n",
    "        - country_counts (dict): Frequency table of cat breeds per country\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(cats_api_url)  # Send GET request to the API\n",
    "    cats = response.json()  # Convert JSON response to Python dictionary/list\n",
    "\n",
    "    # Lists to store numerical data\n",
    "    weights = []  # Stores average weights of breeds\n",
    "    lifespans = []  # Stores average lifespans of breeds\n",
    "    country_counts = {}  # Dictionary to count occurrences of each country\n",
    "\n",
    "    # Loop through each cat breed in the API response\n",
    "    for cat in cats:\n",
    "        # Extract weight if available, convert to float\n",
    "        if 'weight' in cat and 'metric' in cat['weight']:\n",
    "            weight_range = list(map(float, cat['weight']['metric'].split(' - ')))  # Convert \"3 - 5\" to [3.0, 5.0]\n",
    "            weights.append(statistics.mean(weight_range))  # Store the average weight\n",
    "\n",
    "        # Extract lifespan if available, convert to float\n",
    "        if 'life_span' in cat:\n",
    "            lifespan_range = list(map(float, cat['life_span'].split(' - ')))  # Convert \"12 - 15\" to [12.0, 15.0]\n",
    "            lifespans.append(statistics.mean(lifespan_range))  # Store the average lifespan\n",
    "\n",
    "        # Count occurrences of each country of origin\n",
    "        country = cat.get('origin', 'Unknown')  # Get country of origin or set as 'Unknown'\n",
    "        country_counts[country] = country_counts.get(country, 0) + 1  # Increment count\n",
    "\n",
    "    # Compute statistical measures for weight\n",
    "    weight_stats = {\n",
    "        \"min\": min(weights),  # Minimum weight\n",
    "        \"max\": max(weights),  # Maximum weight\n",
    "        \"mean\": statistics.mean(weights),  # Average weight\n",
    "        \"median\": statistics.median(weights),  # Middle value\n",
    "        \"std_dev\": statistics.stdev(weights),  # Standard deviation\n",
    "    }\n",
    "\n",
    "    # Compute statistical measures for lifespan\n",
    "    lifespan_stats = {\n",
    "        \"min\": min(lifespans),  # Minimum lifespan\n",
    "        \"max\": max(lifespans),  # Maximum lifespan\n",
    "        \"mean\": statistics.mean(lifespans),  # Average lifespan\n",
    "        \"median\": statistics.median(lifespans),  # Middle value\n",
    "        \"std_dev\": statistics.stdev(lifespans),  # Standard deviation\n",
    "    }\n",
    "\n",
    "    # Return the computed statistics and country frequency table\n",
    "    return weight_stats, lifespan_stats, country_counts\n",
    "\n",
    "# Call the function to analyze cat data\n",
    "weights, lifespans, country_counts = analyze_cats()\n",
    "\n",
    "# Print results\n",
    "print(\"Weight Stats:\", weights)  # Display weight statistics\n",
    "print(\"Lifespan Stats:\", lifespans)  # Display lifespan statistics\n",
    "print(\"Country Frequency:\", country_counts)  # Display frequency of cat breeds by country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analyze Countries API Data\n",
    "We will:\n",
    "\n",
    "Fetch country data\n",
    "\n",
    "Find the 10 largest countries\n",
    "\n",
    "Find the 10 most spoken languages\n",
    "\n",
    "Count total languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Largest Countries by Population: [{'country': 'China', 'population': 1402112000}, {'country': 'India', 'population': 1380004385}, {'country': 'United States', 'population': 329484123}, {'country': 'Indonesia', 'population': 273523621}, {'country': 'Pakistan', 'population': 220892331}, {'country': 'Brazil', 'population': 212559409}, {'country': 'Nigeria', 'population': 206139587}, {'country': 'Bangladesh', 'population': 164689383}, {'country': 'Russia', 'population': 144104080}, {'country': 'Mexico', 'population': 128932753}]\n",
      "10 Most Spoken Languages: [('English', 91), ('French', 46), ('Arabic', 25), ('Spanish', 24), ('Portuguese', 10), ('Dutch', 7), ('Russian', 7), ('German', 6), ('Chinese', 5), ('Italian', 4)]\n",
      "Total Number of Unique Languages: 155\n"
     ]
    }
   ],
   "source": [
    "import requests  # Import requests to fetch API data\n",
    "\n",
    "def analyze_countries():\n",
    "    \"\"\"\n",
    "    Fetches country data from the REST Countries API and analyzes:\n",
    "    - The 10 largest countries by population\n",
    "    - The 10 most spoken languages globally\n",
    "    - The total number of unique languages spoken\n",
    "    \n",
    "    Returns:\n",
    "        - largest_countries (list of dicts): Top 10 countries with their population\n",
    "        - most_spoken_languages (list of tuples): Top 10 languages with their frequency\n",
    "        - total_languages (int): Total number of unique languages spoken worldwide\n",
    "    \"\"\"\n",
    "\n",
    "    # API URL for retrieving country data\n",
    "    countries_api_url = 'https://restcountries.com/v3.1/all'\n",
    "\n",
    "    # Send a GET request to fetch country data\n",
    "    response = requests.get(countries_api_url)\n",
    "    countries = response.json()  # Convert the API response to a Python list\n",
    "\n",
    "    # Extract the 10 largest countries by population\n",
    "    largest_countries = sorted(\n",
    "        countries, \n",
    "        key=lambda c: c.get('population', 0),  # Use 0 as default if 'population' is missing\n",
    "        reverse=True  # Sort in descending order\n",
    "    )[:10]  # Take the top 10\n",
    "\n",
    "    # Format the largest countries into a list of dictionaries\n",
    "    largest_countries = [\n",
    "        {'country': c['name']['common'], 'population': c['population']} \n",
    "        for c in largest_countries\n",
    "    ]\n",
    "\n",
    "    # Dictionary to count occurrences of languages\n",
    "    language_counts = {}\n",
    "\n",
    "    # Set to store unique languages\n",
    "    all_languages = set()\n",
    "\n",
    "    # Loop through each country in the dataset\n",
    "    for country in countries:\n",
    "        languages = country.get('languages', {})  # Get the 'languages' dictionary, default to empty\n",
    "        for lang in languages.values():  # Iterate through language names\n",
    "            language_counts[lang] = language_counts.get(lang, 0) + 1  # Count occurrences\n",
    "            all_languages.add(lang)  # Store unique languages\n",
    "\n",
    "    # Get the 10 most spoken languages (sorted by occurrence count)\n",
    "    most_spoken_languages = sorted(\n",
    "        language_counts.items(), \n",
    "        key=lambda x: x[1],  # Sort by frequency\n",
    "        reverse=True  # Sort in descending order\n",
    "    )[:10]  # Take the top 10\n",
    "\n",
    "    # Return results: largest countries, most spoken languages, and total unique languages\n",
    "    return largest_countries, most_spoken_languages, len(all_languages)\n",
    "\n",
    "# Call the function to analyze country data\n",
    "largest_countries, top_languages, total_languages = analyze_countries()\n",
    "\n",
    "# Display results\n",
    "print(\"10 Largest Countries by Population:\", largest_countries)\n",
    "print(\"10 Most Spoken Languages:\", top_languages)\n",
    "print(\"Total Number of Unique Languages:\", total_languages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scrape UCI Machine Learning Repository\n",
    "We will:\n",
    "\n",
    "Fetch and parse the webpage\n",
    "\n",
    "Extract dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI Datasets: ['Request failed: 404 Client Error: Not Found for url: https://archive.ics.uci.edu/datasets.php']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape dataset names from UCI Machine Learning Repository\n",
    "def scrape_uci_datasets():\n",
    "    \"\"\"\n",
    "    Scrapes dataset names from the UCI Machine Learning Repository.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of the first 10 dataset names.\n",
    "    \"\"\"\n",
    "    uci_url = 'https://archive.ics.uci.edu/ml/datasets.php'  # UCI dataset URL\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}  # Set headers to avoid bot blocking\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(uci_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise error if request fails\n",
    "    except requests.Timeout:\n",
    "        return [\"Error: Request timed out\"]\n",
    "    except requests.RequestException as e:\n",
    "        return [f\"Request failed: {e}\"]\n",
    "    \n",
    "    # Parse HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract dataset names\n",
    "    datasets = []\n",
    "    tables = soup.find_all(\"table\")  # Find all tables on the page\n",
    "    if tables:\n",
    "        for table in tables:\n",
    "            for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "                columns = row.find_all(\"td\")\n",
    "                if columns and columns[0].a:  # Ensure the first column has a link\n",
    "                    dataset_name = columns[0].a.text.strip()\n",
    "                    datasets.append(dataset_name)\n",
    "    \n",
    "    return datasets[:10] if datasets else [\"No datasets found\"]\n",
    "\n",
    "# Display the scraped datasets\n",
    "print(\"UCI Datasets:\", scrape_uci_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (4.30.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.13.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python exercise focuses on retrieving and analyzing various types of data. (Explains the purpose of the task) It includes processing text from Romeo and Juliet to identify frequently used words. (Summarizes the text analysis task) Additionally, it involves interacting with APIs, such as the Cats API, to compute key statistics on cat weights, lifespans, and breed distributions. (Describes working with APIs and extracting meaningful insights) The Countries API is also explored to determine the largest nations, most commonly spoken languages, and the total number of languages recorded. (Highlights geographic and linguistic data analysis) Lastly, the exercise introduces web scraping using BeautifulSoup4 to collect dataset information from the UCI Machine Learning Repository. (Explains the web scraping task) Overall, these tasks strengthen skills in data collection, structured and unstructured data handling, API usage, statistical calculations, and web scraping. (Emphasizes the skill-building aspect of the exercise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
